{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from stopwordsiso import stopwords\n",
    "from telethon import TelegramClient\n",
    "import nest_asyncio\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Путь к JSON-файлу с данными новостей (при необходимости измените)\n",
    "JSON_PATH = \"news_data2.json\"\n",
    "\n",
    "# Директория для сохранения изображений (создаётся автоматически, если не существует)\n",
    "SAVE_DIR = \"/Applications/Study/Diploma/images\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Параметры доступа к API Telegram\n",
    "api_id = 25033293        # API ID вашего приложения\n",
    "api_hash = \"7af4a394d50e40a4b7475d5c87d110dd\"  # API Hash вашего приложения\n",
    "\n",
    "# Список каналов, сгруппированных по категориям\n",
    "channels = [\n",
    "    [\"@rian_ru\"],\n",
    "    [\"@sportrian\", \"@sportsru\", \"@UFCRussia\", \"@football_nik\", \"@hockey_vbros\"],\n",
    "    [\"@rianovostiAmerica\", \"@politica_media\", \"@vv_volodin\", \"@margaritasimonyan\"],\n",
    "    [\"@World_Sanctions\"],\n",
    "    [\"@igmtv\", \"@inQsitor\"],\n",
    "    [\"@movie7channel\", \"@marvel4\"],\n",
    "    [\"@rasofficial\"]\n",
    "]\n",
    "\n",
    "# Соответствующие им названия категорий\n",
    "categories = [\"General news\", \"Sport\", \"Politic\", \"Economic\", \"Games\", \"Films and serials\", \"Science\"]\n",
    "\n",
    "# Карта канала → категория для быстрого присвоения\n",
    "channel_to_category = {}\n",
    "for cat, ch_list in zip(categories, channels):\n",
    "    for ch in ch_list:\n",
    "        channel_to_category[ch] = cat\n",
    "\n",
    "# Загрузка моделей эмбеддингов (SentenceTransformer, CLIP, BLIP)\n",
    "print(\"Загрузка моделей эмбеддингов (SentenceTransformer, CLIP, BLIP)...\")\n",
    "text_encoder = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "clip_model     = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "blip_model     = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "print(\"Модели загружены. Запуск цикла обновления...\")\n",
    "\n",
    "# Пути к файлам FAISS-индексов и вспомогательным данным\n",
    "TEXT_INDEX_PATH    = \"news_date.faiss\"        # Индекс для текстовых эмбеддингов\n",
    "CLIP_INDEX_PATH    = \"clip_index_last.faiss\"  # Индекс для эмбеддингов изображений (CLIP)\n",
    "BLIP_INDEX_PATH    = \"blip_index_last.faiss\"  # Индекс для эмбеддингов подписей BLIP\n",
    "CLIP_PATHS_PATH    = \"clip_valid_paths.pkl\"   # Сохранённые пути к изображениям (CLIP)\n",
    "BLIP_CAPTIONS_PATH = \"blip_captions.pkl\"      # Сохранённые подписи (BLIP)\n",
    "\n",
    "# Асинхронный основной цикл: соединение, загрузка, проверка и обновление индексов\n",
    "async def update_loop():\n",
    "    # Запуск и подключение клиента Telegram\n",
    "    client = TelegramClient('session_name', api_id, api_hash)\n",
    "    await client.start()\n",
    "    print(\"Клиент Telegram запущен и подключен.\")\n",
    "\n",
    "    # Загрузка существующих данных или инициализация пустых структур\n",
    "    if os.path.exists(JSON_PATH):\n",
    "        with open(JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            existing_data = json.load(f)\n",
    "        # Проверим, что все ключи есть даже в пустом файле\n",
    "        for key in [\"category\", \"channel\", \"message_id\", \"time\", \"text\", \"image_path\"]:\n",
    "            existing_data.setdefault(key, [])\n",
    "    else:\n",
    "        existing_data = {\"category\": [], \"channel\": [], \"message_id\": [], \"time\": [], \"text\": [], \"image_path\": []}\n",
    "\n",
    "    # Определение последнего обработанного ID для каждого канала\n",
    "    last_ids = {}\n",
    "    if existing_data[\"channel\"]:\n",
    "        for ch, msg_id in zip(existing_data[\"channel\"], existing_data[\"message_id\"]):\n",
    "            last_ids[ch] = max(last_ids.get(ch, 0), msg_id)\n",
    "    else:\n",
    "        # Если данных нет, установим 0, чтобы получить начальную выборку\n",
    "        last_ids = {ch: 0 for ch_list in channels for ch in ch_list}\n",
    "\n",
    "    # Если JSON был пуст — первичная загрузка по 400 сообщений из каждого канала\n",
    "    if not existing_data[\"message_id\"]:\n",
    "        print(\"Первичная загрузка данных из всех каналов...\")\n",
    "        for cat, ch_list in zip(categories, channels):\n",
    "            for ch_name in ch_list:\n",
    "                try:\n",
    "                    entity = await client.get_entity(ch_name)\n",
    "                    msgs = await client.get_messages(entity, limit=400)\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при загрузке initial-сообщений из {ch_name}: {e}\")\n",
    "                    continue\n",
    "                for msg in msgs:\n",
    "                    existing_data[\"category\"].append(cat)\n",
    "                    existing_data[\"channel\"].append(ch_name)\n",
    "                    existing_data[\"message_id\"].append(msg.id)\n",
    "                    existing_data[\"time\"].append(msg.date.strftime(\"%d.%m.%Y\"))\n",
    "                    existing_data[\"text\"].append(msg.message)\n",
    "                    if msg.photo:\n",
    "                        path = os.path.join(SAVE_DIR, f\"{ch_name[1:]}_{msg.id}.jpg\")\n",
    "                        try:\n",
    "                            await msg.download_media(file=path)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Не удалось скачать изображение {path}: {e}\")\n",
    "                            path = \"\"\n",
    "                        existing_data[\"image_path\"].append(path)\n",
    "                    else:\n",
    "                        existing_data[\"image_path\"].append(\"\")\n",
    "                if msgs:\n",
    "                    last_ids[ch_name] = max(m.id for m in msgs)\n",
    "        # Сохраняем начальные данные\n",
    "        with open(JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Сохранено {len(existing_data['message_id'])} записей в {JSON_PATH}.\")\n",
    "\n",
    "    # Кэш объектов каналов, чтобы не делать повторные запросы\n",
    "    channel_entities = {}\n",
    "    iteration = 0\n",
    "\n",
    "    # Вечный цикл: проверяем новые сообщения, обновляем JSON и FAISS-индексы\n",
    "    while True:\n",
    "        # Загрузим текущие индексы\n",
    "        text_index = faiss.read_index(TEXT_INDEX_PATH)\n",
    "        clip_index = faiss.read_index(CLIP_INDEX_PATH)\n",
    "        blip_index = faiss.read_index(BLIP_INDEX_PATH)\n",
    "\n",
    "        # Собираем новые записи\n",
    "        new_entries = {\"category\": [], \"channel\": [], \"message_id\": [], \"time\": [], \"text\": [], \"image_path\": []}\n",
    "        for cat, ch_list in zip(categories, channels):\n",
    "            for ch_name in ch_list:\n",
    "                if ch_name not in channel_entities:\n",
    "                    try:\n",
    "                        channel_entities[ch_name] = await client.get_entity(ch_name)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Не удалось получить сущность {ch_name}: {e}\")\n",
    "                        continue\n",
    "                entity = channel_entities[ch_name]\n",
    "                last_known = last_ids.get(ch_name, 0)\n",
    "                try:\n",
    "                    if last_known > 0:\n",
    "                        msgs = await client.get_messages(entity, min_id=last_known, limit=None)\n",
    "                    else:\n",
    "                        msgs = await client.get_messages(entity, limit=100)\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при получении сообщений из {ch_name}: {e}\")\n",
    "                    continue\n",
    "                msgs = sorted(msgs, key=lambda m: m.id)\n",
    "                for msg in msgs:\n",
    "                    if last_known and msg.id <= last_known:\n",
    "                        continue\n",
    "                    new_entries[\"category\"].append(cat)\n",
    "                    new_entries[\"channel\"].append(ch_name)\n",
    "                    new_entries[\"message_id\"].append(msg.id)\n",
    "                    new_entries[\"time\"].append(msg.date.strftime(\"%d.%m.%Y\"))\n",
    "                    new_entries[\"text\"].append(msg.message)\n",
    "                    if msg.photo:\n",
    "                        path = os.path.join(SAVE_DIR, f\"{ch_name[1:]}_{msg.id}.jpg\")\n",
    "                        try:\n",
    "                            await msg.download_media(file=path)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Не удалось скачать новое изображение {path}: {e}\")\n",
    "                            path = \"\"\n",
    "                        new_entries[\"image_path\"].append(path)\n",
    "                    else:\n",
    "                        new_entries[\"image_path\"].append(\"\")\n",
    "                    last_ids[ch_name] = max(last_known, msg.id)\n",
    "\n",
    "        # Если пришли новые данные, добавляем их в JSON и обновляем индексы\n",
    "        if new_entries[\"message_id\"]:\n",
    "            for k in new_entries:\n",
    "                existing_data[k].extend(new_entries[k])\n",
    "            with open(JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "                json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Добавлено {len(new_entries['message_id'])} новых записей, всего теперь {len(existing_data['message_id'])}.\")\n",
    "\n",
    "            # Циклически обновляем по одному индексу за проход\n",
    "            for idx in range(3):\n",
    "                if idx == 0:\n",
    "                    # Текстовый индекс\n",
    "                    print(\"Обновление текстового FAISS-индекса...\")\n",
    "                    df = pd.DataFrame(new_entries)\n",
    "                    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n",
    "                    df = df[df[\"text\"].str.strip().ne(\"\")].reset_index(drop=True)\n",
    "                    ru_stop = stopwords(\"ru\")\n",
    "                    def clean_text(t: str) -> str:\n",
    "                        txt = BeautifulSoup(t, \"html.parser\").get_text(\" \", strip=True)\n",
    "                        txt = unicodedata.normalize(\"NFKC\", txt).lower()\n",
    "                        txt = re.sub(r\"[^\\w\\s]\", \" \", txt)\n",
    "                        return \" \".join([w for w in txt.split() if w not in ru_stop])\n",
    "                    df[\"clean_text\"] = df[\"text\"].map(clean_text)\n",
    "                    df[\"wc\"] = df[\"clean_text\"].str.split().apply(len)\n",
    "                    df = df[df[\"wc\"] > 7].reset_index(drop=True)\n",
    "                    df.drop(columns=[\"wc\"], inplace=True)\n",
    "                    months = {\n",
    "                        '01': 'января','02': 'февраля','03': 'марта','04': 'апреля',\n",
    "                        '05': 'мая','06': 'июня','07': 'июля','08': 'августа',\n",
    "                        '09': 'сентября','10': 'октября','11': 'ноября','12': 'декабря'\n",
    "                    }\n",
    "                    df[\"date_ru\"] = df[\"time\"].str.replace(\n",
    "                        r\"(\\\\d{2})\\\\.(\\\\d{2})\\\\.\\\\d{4}\",\n",
    "                        lambda m: f\"{int(m.group(1))} {months[m.group(2)]}\",\n",
    "                        regex=True\n",
    "                    )\n",
    "                    texts = (df[\"date_ru\"] + \" \" + df[\"clean_text\"]).tolist()\n",
    "                    emb = text_encoder.encode(texts, show_progress_bar=False)\n",
    "                    vecs = np.array(emb, dtype=\"float32\")\n",
    "                    faiss.normalize_L2(vecs)\n",
    "                    text_index.add(vecs)\n",
    "                    faiss.write_index(text_index, TEXT_INDEX_PATH)\n",
    "                    print(f\"Текстовый индекс: {text_index.ntotal} векторов.\")\n",
    "                elif idx == 1:\n",
    "                    # CLIP-индекс (изображения)\n",
    "                    print(\"Обновление CLIP-индекса изображений...\")\n",
    "                    paths = [p for p in new_entries[\"image_path\"] if p]\n",
    "                    embs, valid = [], []\n",
    "                    for p in paths:\n",
    "                        try:\n",
    "                            img = Image.open(p).convert(\"RGB\")\n",
    "                            inp = clip_processor(images=img, return_tensors=\"pt\")\n",
    "                            with torch.no_grad():\n",
    "                                im_emb = clip_model.get_image_features(**inp)\n",
    "                            arr = im_emb.cpu().numpy().astype(\"float32\")[0]\n",
    "                            embs.append(arr)\n",
    "                            valid.append(p)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Ошибка CLIP для {p}: {e}\")\n",
    "                    if embs:\n",
    "                        arr = np.vstack(embs)\n",
    "                        clip_index.add(arr)\n",
    "                        faiss.write_index(clip_index, CLIP_INDEX_PATH)\n",
    "                        with open(CLIP_PATHS_PATH, \"wb\") as f:\n",
    "                            pickle.dump(valid, f)\n",
    "                        print(f\"CLIP-индекс: {clip_index.ntotal} изображений.\")\n",
    "                else:\n",
    "                    # BLIP-индекс (автоматические подписи)\n",
    "                    print(\"Обновление BLIP-индекса подписей...\")\n",
    "                    paths = [p for p in new_entries[\"image_path\"] if p]\n",
    "                    caps, valid = [], []\n",
    "                    for p in paths:\n",
    "                        try:\n",
    "                            img = Image.open(p).convert(\"RGB\")\n",
    "                            inp = blip_processor(images=img, return_tensors=\"pt\")\n",
    "                            with torch.no_grad():\n",
    "                                outs = blip_model.generate(**inp)\n",
    "                            cap = blip_processor.batch_decode(outs, skip_special_tokens=True)[0].strip()\n",
    "                            caps.append(cap)\n",
    "                            valid.append(p)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Ошибка BLIP для {p}: {e}\")\n",
    "                    if caps:\n",
    "                        emb_caps = text_encoder.encode(caps, show_progress_bar=False)\n",
    "                        arr = np.array(emb_caps, dtype=\"float32\")\n",
    "                        faiss.normalize_L2(arr)\n",
    "                        blip_index.add(arr)\n",
    "                        faiss.write_index(blip_index, BLIP_INDEX_PATH)\n",
    "                        with open(BLIP_CAPTIONS_PATH, \"wb\") as f:\n",
    "                            pickle.dump(caps, f)\n",
    "                        print(f\"BLIP-индекс: {blip_index.ntotal} подписей.\")\n",
    "            print(f\"Итерация {iteration} завершена.\\n{'-'*60}\")\n",
    "            iteration += 1\n",
    "\n",
    "        # Пауза перед следующей проверкой\n",
    "        await asyncio.sleep(300)\n",
    "\n",
    "# Запуск\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    try:\n",
    "        asyncio.run(update_loop())\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Автоматизация остановлена пользователем.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
